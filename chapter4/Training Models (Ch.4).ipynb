{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the topics discussed in this chapter will be essential in understanding, building, and training neural networks.\n",
    "\n",
    "We will start by looking at the Linear Regression model, one of the simplest models there is. We will discuss two very different ways to train it:\n",
    "\n",
    "- Using a direct “closed-form” equation that directly computes the model parameters that best fit the model to the training set.\n",
    "- Using an iterative optimization approach, Gradient Descent (GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method.\n",
    "\n",
    "Next we will look at Polynomial Regression, a more complex model that can fit non‐ linear datasets. Since this model has more parameters than Linear Regression, it is more prone to overfitting the training data, so we will look at how to detect whether or not this is the case, using learning curves, and then we will look at several regulari‐ zation techniques that can reduce the risk of overfitting the training set.\n",
    "Finally, we will look at two more models that are commonly used for classification tasks: Logistic Regression and Softmax Regression.\n",
    "\n",
    "##### 1. Linear Regression\n",
    "\n",
    "A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term).\n",
    "\n",
    "- Predicted value = bias + weighted_input_feat_1 + weighted_input_feat_2 + ... + weighted_input_feat_n\n",
    "\n",
    "Training a model means setting its parameters so that the model best fits the training set. For this purpose, we first need a measure of how well (or poorly) the model fits the training data. The most common performance measure of a regression model is the Root Mean Square Error (RMSE). Therefore, to train a Linear Regression model, you need to find the value of θ (weights) that minimizes the RMSE.\n",
    "\n",
    "##### The normal equation\n",
    "\n",
    "There is a closed-form solution to find the value of θ that minimizes the cost function, called the Normal Equation.\n",
    "- θ=(XTX)^−1 XT y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = 2 * np.random.rand(100, 1) # \n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to generate the data is y = 4 + 3x1 + Gaussian noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 15]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.axis([0, 2, 0, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19621013],\n",
       "       [2.86024227]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y) # inv() function from NumPy’s Linear Algebra module, \n",
    "                                                             # (np.linalg) to compute the inverse of a matrix, \n",
    "                                                             # and the dot() method for matrix multiplication\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would have hoped for θ0 = 4 and θ1 = 3 instead of θ0 = 4.215 and θ1 = 2.770. Close enough, but the noise made it impossible to recover the exact parameters of the original function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19621013],\n",
       "       [9.91669467]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions using theta hat:\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 15]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot predictions\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform linear regression with skclearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.19621013]), array([[2.86024227]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19621013],\n",
       "       [9.91669467]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19621013],\n",
       "       [2.86024227]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    "theta_best_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19621013],\n",
       "       [2.86024227]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed using a standard matrix factorization technique\n",
    "called Singular Value Decomposition (SVD) that can decompose the training set\n",
    "matrix X into the matrix multiplication of three matrices U Σ VT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some features are redundant, but the pseudoinverse is always defined.\n",
    "\n",
    "Now we will look at very different ways to train a Linear Regression model, better suited for cases where there are a large number of features, or too many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Gradient descent\n",
    "\n",
    "The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. It measures the local gradient of the error function with regards to the parameter vector θ, and it goes in the direction of descending gradient. Once the gradient is zero, we have reached a minimum.\n",
    "\n",
    "Concretely, we start by filling θ with random values (random initialization), and then we improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum.\n",
    "\n",
    "An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time. On the other hand, if the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find a good solution.\n",
    "\n",
    "Finally, not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum very difficult.\n",
    "\n",
    "Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if we pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum.\n",
    "\n",
    "Training a model means searching for a combination of model parameters that minimizes a cost function (over the training set). It is a search in the model’s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is: searching for a needle in a 300-dimensional haystack is much trickier than in three dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch gradient descent\n",
    "\n",
    "To implement GD, we need to calculate how much the cost function will change if you change θj just a little bit (a partial derivative). Batch GD uses the whole batch of training data at every step (actually, Full Gradient Descent would probably be a better name). As a result it is terribly slow on very large training sets. However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an implementation:\n",
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.19621013],\n",
       "       [2.86024227]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta # results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stochastic gradient descent\n",
    "\n",
    "Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients based only on that single instance. Obviously this makes the algorithm much faster since it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration.\n",
    "\n",
    "Randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing Stochastic Gradient Descent using a simple learning schedule\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2, 1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_indx = np.random.randint(m)\n",
    "        xi = X_b[random_indx:random_indx + 1]\n",
    "        yi = y[random_indx:random_indx + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.14355353],\n",
       "       [2.87294155]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing SGD with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.19639535]), array([2.88920422]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mini-batch gradient descent\n",
    "\n",
    "Instead of computing the gradients based on the full training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD computes the gradients on small random sets of instances called mini-batches. The main advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware optimization of matrix operations, especially when using GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
